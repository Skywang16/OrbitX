# 需求文档

## 介绍

本项目旨在将 `src/eko-core` 目录中的AI代理系统从使用 ai-sdk 库迁移到使用原生后端接口。当前系统依赖于多个 ai-sdk 包来处理不同的LLM提供商，这增加了复杂性和依赖管理的负担。通过迁移到统一的原生后端接口，我们可以简化架构、提高性能，并获得更好的控制能力。

原生后端基于 Tauri 2.0 实现，提供了统一的LLM调用接口，支持流式和非流式调用，并通过 Channel 进行双向通信。后端已经实现了对多个LLM提供商的支持，包括 OpenAI、Anthropic、Google、Qwen 等。

## 需求

### 需求 1

**用户故事：** 作为开发者，我希望移除对 ai-sdk 库的依赖，以便简化项目的依赖管理和减少包大小。

#### 验收标准

1. 当系统启动时，应该不再加载任何 ai-sdk 相关的包
2. 当检查 package.json 时，应该不包含任何 ai-sdk 相关的依赖
3. 当编译项目时，应该不会出现 ai-sdk 相关的导入错误
4. 当运行项目时，应该能够正常工作而不依赖 ai-sdk

### 需求 2

**用户故事：** 作为开发者，我希望使用统一的原生后端接口来调用LLM，以便获得更好的性能和控制能力。

#### 验收标准

1. 当调用LLM时，应该通过 Tauri 命令 `llm_call` 进行非流式调用
2. 当需要流式响应时，应该通过 Tauri 命令 `llm_call_stream` 和 Channel 进行流式调用
3. 当调用失败时，应该能够正确处理和传播错误信息
4. 当取消请求时，应该能够正确中止后端的LLM调用

### 需求 3

**用户故事：** 作为开发者，我希望保持现有的API接口不变，以便现有代码能够继续工作而无需大量修改。

#### 验收标准

1. 当现有代码调用LLM相关函数时，应该能够正常工作
2. 当使用工具调用功能时，应该能够正确处理工具的调用和结果
3. 当处理流式数据时，应该能够正确解析和分发流式事件
4. 当使用内存压缩功能时，应该能够正常工作

### 需求 4

**用户故事：** 作为开发者，我希望类型系统能够正确反映新的原生接口，以便获得良好的开发体验和类型安全。

#### 验收标准

1. 当编写代码时，应该能够获得正确的类型提示和自动完成
2. 当编译代码时，应该不会出现类型错误
3. 当使用新的原生类型时，应该与后端的 Rust 类型保持一致
4. 当进行类型转换时，应该能够正确处理数据格式的差异

### 需求 5

**用户故事：** 作为开发者，我希望流式数据处理能够正确工作，以便用户能够实时看到LLM的响应。

#### 验收标准

1. 当LLM开始生成内容时，应该能够实时接收到文本增量
2. 当LLM进行推理时，应该能够接收到推理过程的流式数据
3. 当LLM调用工具时，应该能够接收到工具调用的流式事件
4. 当流式调用完成时，应该能够接收到完成事件和使用统计

### 需求 6

**用户故事：** 作为开发者，我希望工具调用功能能够正确工作，以便AI代理能够执行各种操作。

#### 验收标准

1. 当AI需要调用工具时，应该能够正确解析工具调用请求
2. 当工具执行完成时，应该能够正确格式化工具结果
3. 当工具调用失败时，应该能够正确处理错误情况
4. 当有多个工具调用时，应该能够正确处理并发调用

### 需求 7

**用户故事：** 作为开发者，我希望错误处理机制能够正确工作，以便能够诊断和解决问题。

#### 验收标准

1. 当后端返回错误时，应该能够正确解析和传播错误信息
2. 当网络连接失败时，应该能够提供有意义的错误消息
3. 当模型不可用时，应该能够尝试备用模型或返回适当的错误
4. 当请求超时时，应该能够正确处理超时情况

### 需求 8

**用户故事：** 作为开发者，我希望性能能够得到改善，以便用户获得更好的体验。

#### 验收标准

1. 当进行LLM调用时，响应时间应该不超过原有实现
2. 当处理流式数据时，延迟应该保持在可接受的范围内
3. 当系统运行时，内存使用应该不超过原有实现
4. 当进行大量并发调用时，系统应该能够稳定运行
